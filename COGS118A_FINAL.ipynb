{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BbYJ_vRTXbK"
      },
      "source": [
        "Sahib Athwal\\\n",
        "A13450589\\\n",
        "COGS 118A: Final\\\n",
        "Professor Fleischer\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOVyrd18Qt6x"
      },
      "source": [
        "# **Experiment Overview**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwHitenySSY5"
      },
      "source": [
        "This section will run through each of the respective supervised machine learning algorithms that include: Decision Tree, Random Forests, and KNN Models. The results will be displayed for each of the respective datasets we are running our algorithms on. The implementation of each of the supervised machine learning techniques will utilize python libraries stated below in our implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krUIJ2HqL2hE"
      },
      "source": [
        "#### **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j76GS3Qd6NAs"
      },
      "outputs": [],
      "source": [
        "# Import for reading and manipulating data\n",
        "import pandas as pd\n",
        "\n",
        "# Numerical operations\n",
        "import numpy as np\n",
        "\n",
        "# Importing stats module for calculating z-scores, mean, etc.\n",
        "from scipy import stats\n",
        "\n",
        "# Importing seaborn for data visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# Importing scikit-learn modules\n",
        "from sklearn.svm import SVR  # Support Vector Regression\n",
        "from sklearn import tree  # Decision Tree\n",
        "from sklearn import preprocessing  # Preprocessing tools\n",
        "from sklearn.model_selection import GridSearchCV  # Grid Search for hyperparameter tuning\n",
        "from sklearn.preprocessing import LabelEncoder  # Encoding categorical variables\n",
        "#from sklearn.preprocessing import MultiColumnLabelEncoder  # Encoding categorical variables\n",
        "from sklearn.preprocessing import OneHotEncoder  # One-hot encoding categorical variables\n",
        "from sklearn.compose import ColumnTransformer  # Combining transformers for different data types\n",
        "from sklearn.preprocessing import StandardScaler  # Scaling features\n",
        "from sklearn.feature_selection import SelectKBest, f_classif  # Feature selection\n",
        "from sklearn.utils import shuffle  # Shuffling data\n",
        "from sklearn.model_selection import cross_val_score  # Cross-validation\n",
        "from sklearn import svm  # Support Vector Machines\n",
        "from sklearn.tree import DecisionTreeClassifier  # Decision Tree Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier  # Random Forest Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier  # K-Nearest Neighbors Classifier\n",
        "from sklearn.datasets import make_classification  # Creating classification datasets\n",
        "from sklearn.linear_model import LogisticRegression  # Logistic Regression\n",
        "from sklearn.model_selection import train_test_split  # Splitting data into train and test sets\n",
        "from sklearn.pipeline import make_pipeline  # Creating pipelines\n",
        "\n",
        "# Importing matplotlib for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'retina'  # High-resolution figures\n",
        "\n",
        "# Import for reading and manipulating data\n",
        "import scipy.io as sio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opftsNq7o3oz"
      },
      "source": [
        "## **Classifier Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMdhDuIDWkzM"
      },
      "source": [
        "#### **Heat Map Visualiztion**\n",
        "\n",
        "This is the helper method that allows us to view each of our respective classifiers as a heat map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMwSrYOUo-fR"
      },
      "outputs": [],
      "source": [
        "global count \n",
        "count = 0\n",
        "def draw_heatmap(acc, acc_desc, C_list, character):\n",
        "    global count\n",
        "    plt.figure(figsize = (2,4))\n",
        "    ax = sns.heatmap(acc, annot=True, fmt='.3f', yticklabels=C_list, \n",
        "                     xticklabels=[])\n",
        "    ax.collections[0].colorbar.set_label(\"accuracy\")\n",
        "    ax.set(ylabel='$'  + character + '$')\n",
        "    plt.title(acc_desc + ' w.r.t $' + character + '$')\n",
        "    sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "    plt.show()\n",
        "    count+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69gKebHuWj-M"
      },
      "source": [
        "#### **Linear Support Vector Machine Classifier**\n",
        "\n",
        "This is the helper method is designed to perform binary classifcation using a Support Vector Machine (SVN) algorithm with a linear kernel. It aims to find the optimal value of the regularization parameter 'C' for the SVM model by performing a grid search and k-fold cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJx1RHxIpEkS"
      },
      "outputs": [],
      "source": [
        "def svm_func():\n",
        "    # SVM binary classification used linear instead of RBF (Faster runtime)\n",
        "    classifier = svm.SVC(kernel='linear')\n",
        "\n",
        "    # Different C values to try\n",
        "    C_list = [10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 1]\n",
        "    parameters = {'C': C_list}\n",
        "\n",
        "    # Perform a grid Search to identify the best C and to perform K 5 Folds\n",
        "    clf = GridSearchCV(classifier, parameters, return_train_score=True, cv=5)\n",
        "\n",
        "    # Have to fit the classifier with the training data\n",
        "    clf.fit(X_train_val, Y_train_val.values.ravel())\n",
        "\n",
        "    # Extract the training and validation accuracies and plot them as heat maps\n",
        "    # to visualize the best C parameter\n",
        "    train_acc = clf.cv_results_['mean_train_score']\n",
        "    draw_heatmap(train_acc.reshape(-1, 1), 'train accuracy', C_list, 'C')\n",
        "\n",
        "    val_acc = clf.cv_results_['mean_test_score']\n",
        "    draw_heatmap(val_acc.reshape(-1, 1), 'val accuracy', C_list, 'C')\n",
        "\n",
        "    # Find the optimal C parameter and use that to redefine the classifier\n",
        "    optimal_classifier = svm.SVC(kernel='linear', C=clf.best_params_['C'])\n",
        "\n",
        "    for i, j in enumerate(C_list):\n",
        "        if j == clf.best_params_['C']:\n",
        "            best_train_acc = train_acc[i]\n",
        "\n",
        "    # Find test accuracy\n",
        "    optimal_classifier.fit(X_train_val, Y_train_val.values.ravel())\n",
        "    test_acc = optimal_classifier.score(X_test, Y_test.values.ravel())\n",
        "    return test_acc, best_train_acc, clf.best_params_['C']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKIHjo4NWi8M"
      },
      "source": [
        "#### **Decision Tree Classifier**\n",
        "\n",
        "This is the helper method that is designed to perform a grid search and cross validation to find the optimal maximum depth parameter (max_depth) for a Decision Tree Classifier. The function aims to identify the (max_depth) value that yields the best performance on the given classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N94B21fpJIq"
      },
      "outputs": [],
      "source": [
        "def decision_Tree():\n",
        "    D_list = np.array([1, 2, 3, 4, 5])\n",
        "    parameters = {'max_depth':D_list}\n",
        "\n",
        "    classifier_grid = GridSearchCV(DecisionTreeClassifier(criterion=\"entropy\"), \n",
        "                                   parameters, cv=5, return_train_score=True)\n",
        "    \n",
        "    #Have to fit the classifier with the training data\n",
        "    classifier_grid.fit(X_train_val, Y_train_val)\n",
        "\n",
        "    #Show the heatmaps\n",
        "    draw_heatmap(classifier_grid.cv_results_['mean_train_score'].reshape(5,1), \n",
        "                 'DT train accuracy', D_list, 'D')\n",
        "    draw_heatmap(classifier_grid.cv_results_['mean_test_score'].reshape(5,1), \n",
        "                 'DT val accuracy', D_list, 'D')\n",
        "    \n",
        "    #Train and Test with best parameters\n",
        "    D_star = classifier_grid.best_params_['max_depth']\n",
        "    classifier_test = DecisionTreeClassifier(max_depth=D_star, \n",
        "                                             criterion=\"entropy\")\n",
        "    classifier_test.fit(X_train_val, Y_train_val)\n",
        "    Desicion_test_acc = classifier_test.score(X_test,Y_test_val)\n",
        "    \n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score']\n",
        "    \n",
        "    for i,j in enumerate(D_list):\n",
        "        if j == D_star:\n",
        "            best_train_acc = train_acc[i]\n",
        "    \n",
        "    return Desicion_test_acc, best_train_acc, D_star"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qqjk4EqWNuE"
      },
      "source": [
        "#### **Random Forest Classifier**\n",
        "\n",
        "This is the helper method that allows us to perform a grid search and cross-validation tofind the optimal maximum depth for a Random Forest Classifier. This function aims to identify the max_depth value that yields the best performance on the given classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aH5iu9GepSaj"
      },
      "outputs": [],
      "source": [
        "def rand_Forest():\n",
        "    D_list = np.array([1, 2, 3, 4, 5])\n",
        "    parameters = {'max_depth':D_list}\n",
        "\n",
        "    #Tried various parameters on the docs this got me the fastest result using\n",
        "    #K5 Folds specified\n",
        "    classifier_grid = GridSearchCV(RandomForestClassifier(criterion=\"entropy\"), \n",
        "                                   parameters, cv=5, return_train_score=True)\n",
        "    classifier_grid.fit(X_train_val, Y_train_val)\n",
        "\n",
        "    #Show Heatmaps\n",
        "    draw_heatmap(classifier_grid.cv_results_['mean_train_score'].reshape(5,1), \n",
        "                 'RF train accuracy', D_list, 'K')\n",
        "    draw_heatmap(classifier_grid.cv_results_['mean_test_score'].reshape(5,1), \n",
        "                 'RF val accuracy', D_list, 'K')\n",
        "\n",
        "    #Train and Test with best parameters\n",
        "    D_star = classifier_grid.best_params_['max_depth']\n",
        "\n",
        "    #Entropy worked better than the default gini\n",
        "    classifier_test1 = RandomForestClassifier(max_depth=D_star, \n",
        "                                              criterion=\"entropy\")\n",
        "    classifier_test1.fit(X_train_val, Y_train_val)\n",
        "    randForest_acc = classifier_test1.score(X_test,Y_test_val)\n",
        "    \n",
        "    \n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score']\n",
        "    for i,j in enumerate(D_list):\n",
        "        if j == D_star:\n",
        "            best_train_acc = train_acc[i]\n",
        "    \n",
        "    return randForest_acc, best_train_acc, D_star"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtvQ6aVOpda_"
      },
      "source": [
        "#### **KNN Classifier**\n",
        "\n",
        "This is the helper method that allows us to run a grid search and cross-validation to find the optimal number of neighbors parameter for a K-Nearest Neighbors (KNN) classifier. The function aims to identify the n_neighbors parameter that results in the highest accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXbYPXlepWEq"
      },
      "outputs": [],
      "source": [
        "\n",
        "def knn_classifier():\n",
        "    k_list = np.array([1, 2, 3, 4, 5, 6])\n",
        "    parameters = {'n_neighbors':k_list}\n",
        "    classifier_grid = GridSearchCV(KNeighborsClassifier(), parameters, cv=5, \n",
        "                                   return_train_score=True)\n",
        "    classifier_grid.fit(X_train_val, Y_train_val)\n",
        "\n",
        "    #Plot heatmaps for the Training and Testing scores respectively\n",
        "    draw_heatmap(classifier_grid.cv_results_['mean_train_score'].reshape(6,1), \n",
        "                 'KNN train accuracy', k_list, 'K')\n",
        "    draw_heatmap(classifier_grid.cv_results_['mean_test_score'].reshape(6,1), \n",
        "                 'KNN val accuracy', k_list, 'K')\n",
        "\n",
        "    #Train and Test with best parameters\n",
        "    k_star = classifier_grid.best_params_['n_neighbors']\n",
        "    classifier_test2 = KNeighborsClassifier(n_neighbors=k_star)\n",
        "    classifier_test2.fit(X_train_val,Y_train_val)\n",
        "    knn_acc = classifier_test2.score(X_test,Y_test_val)\n",
        "    \n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score']\n",
        "    for i,j in enumerate(k_list):\n",
        "        if j == k_star:\n",
        "            best_train_acc = train_acc[i]\n",
        "    \n",
        "    return knn_acc, best_train_acc, k_star"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jgnufeZ6nYk"
      },
      "source": [
        "## **Bankruptcy Dataset**\n",
        "\n",
        "This given dataset provides 96 various attributes that give us the necessary information to predict whether a company will go bankrupt or not. The reasoning behind using this dataset is for a basis for a real\n",
        "world scenario, where a company may exhibit similarities to these features and may need to take decisive action if they are going to become bankrupt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJII8G1hRm28"
      },
      "source": [
        "### **Cleaning Bankruptcy Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "id": "qADZo9Ki0htE",
        "outputId": "b7eaeb51-9178-4316-adbc-6447c2eaf3ef"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "bankrupcy_data_preserved = pd.read_csv('Bankrupcy.csv')\n",
        "bankrupcy_data = bankrupcy_data_preserved.copy()  # Create a copy for manipulation\n",
        "\n",
        "# Handle missing values (e.g., impute with mean)\n",
        "bankrupcy_data = bankrupcy_data.fillna(bankrupcy_data.mean())\n",
        "\n",
        "# Drop columns with a high percentage of missing values (if any)\n",
        "bankrupcy_data = bankrupcy_data.dropna(axis=1, thresh=bankrupcy_data.shape[0] * 0.8)\n",
        "\n",
        "# Drop duplicate entries (if any)\n",
        "bankrupcy_data.drop_duplicates(inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Cleaning Bankruptcy Dataset: Dealing with outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify outliers based on the Interquartile Range (IQR)\n",
        "def detect_outliers(data, features):\n",
        "    outlier_indices = []\n",
        "    for feature in features:\n",
        "        Q1 = data[feature].quantile(0.25)\n",
        "        Q3 = data[feature].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        outlier_indices.extend(data[(data[feature] < lower_bound) | (data[feature] > upper_bound)].index)\n",
        "    return outlier_indices\n",
        "\n",
        "# Replace outliers with the median or a specified value\n",
        "def handle_outliers(data, features, strategy='median', value=None):\n",
        "    if strategy == 'median':\n",
        "        for feature in features:\n",
        "            median = data[feature].median()\n",
        "            data[feature] = data[feature].apply(lambda x: median if x in list(data[data[feature].isin(data.loc[detect_outliers(data, features), feature])][feature]) else x)\n",
        "    elif strategy == 'value':\n",
        "        if value is None:\n",
        "            raise ValueError(\"You must specify a value when using the 'value' strategy.\")\n",
        "        for feature in features:\n",
        "            data[feature] = data[feature].apply(lambda x: value if x in list(data[data[feature].isin(data.loc[detect_outliers(data, features), feature])][feature]) else x)\n",
        "    return data\n",
        "\n",
        "# Example usage\n",
        "numerical_features = bankrupcy_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Identify outliers\n",
        "outlier_indices = detect_outliers(bankrupcy_data, numerical_features)\n",
        "\n",
        "# Replace outliers with the median\n",
        "bankrupcy_data = handle_outliers(bankrupcy_data, numerical_features, strategy='median')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print the column names\n",
        "print(\"Column names:\")\n",
        "print(bankrupcy_data.columns)\n",
        "\n",
        "# Print the first few rows of the dataset\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(bankrupcy_data.head())\n",
        "\n",
        "# Print the data types of the columns\n",
        "print(\"\\nData types:\")\n",
        "print(bankrupcy_data.dtypes)\n",
        "\n",
        "# Print the unique values in each column\n",
        "print(\"\\nUnique values in each column:\")\n",
        "for column in bankrupcy_data.columns:\n",
        "    print(f\"{column}: {bankrupcy_data[column].unique()}\")\n",
        "\n",
        "# Print the summary statistics of the numerical columns\n",
        "print(\"\\nSummary statistics of numerical columns:\")\n",
        "print(bankrupcy_data.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate numerical and categorical features\n",
        "numerical_features = bankrupcy_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = bankrupcy_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Encode categorical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Split the dataset into features (X) and target (y)\n",
        "X = bankrupcy_data.drop('Bankrupt?', axis=1)  # Features\n",
        "y = bankrupcy_data['Bankrupt?'] # Target variable\n",
        "\n",
        "# Feature selection\n",
        "selector = SelectKBest(f_classif, k=10)  # Select the top 10 features based on ANOVA F-value\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5IS6mslxphJL",
        "outputId": "15bef1ef-532f-4791-e1bd-bece7d006e34"
      },
      "outputs": [],
      "source": [
        "# Split Data By 80/20, 50/50, 20/80\n",
        "partitionVal = [0.8, 0.5, 0.2]\n",
        "result_table = np.zeros((3, 7))\n",
        "result_table1 = np.zeros((3, 7))\n",
        "result_table2 = np.zeros((3, 7))\n",
        "\n",
        "for i, partition in enumerate(partitionVal):\n",
        "    print(\"Partition: \", partition)\n",
        "    knn_test_acc = []\n",
        "    rand_forest_test_acc = []\n",
        "    decision_tree_test_acc = []\n",
        "    svm_test_acc = []\n",
        "\n",
        "    NUM_TRIALS = 5\n",
        "    for trial in range(NUM_TRIALS):\n",
        "        # Mix up the data\n",
        "        bankrupcy_data = bankrupcy_data.sample(frac=1).reset_index(drop=True)\n",
        "        # Find the point where to split the data\n",
        "        breakNum = int(partition * len(bankrupcy_data))\n",
        "\n",
        "        X_train_full = bankrupcy_data.loc[0:breakNum]\n",
        "        X_train_val = X_train_full.drop(\"Bankrupt?\", axis=1)\n",
        "        Y_train_val = X_train_full[\"Bankrupt?\"]\n",
        "        X_test_full = bankrupcy_data.loc[breakNum:]\n",
        "        X_test = X_test_full.drop(\"Bankrupt?\", axis=1)\n",
        "        Y_test_val = X_test_full[\"Bankrupt?\"]\n",
        "\n",
        "        # Call the svm classifier\n",
        "        test_acc, best_train0, C0 = svm_func()\n",
        "        svm_test_acc.append(test_acc)\n",
        "\n",
        "        # Call the knn classifier\n",
        "        test_acc, best_train1, C1 = knn_classifier()\n",
        "        knn_test_acc.append(test_acc)\n",
        "\n",
        "        # Call the Decision Tree classifier\n",
        "        test_acc, best_train2, C2 = decision_Tree()\n",
        "        decision_tree_test_acc.append(test_acc)\n",
        "\n",
        "        # Call the Random Forest classifier\n",
        "        test_acc, best_train3, C3 = rand_Forest()\n",
        "        rand_forest_test_acc.append(test_acc)\n",
        "\n",
        "    # result_table[i, 0] = sum(svm_test_acc)/NUM_TRIALS\n",
        "    result_table[i, 1] = sum(knn_test_acc) / NUM_TRIALS\n",
        "    result_table[i, 2] = sum(decision_tree_test_acc) / NUM_TRIALS\n",
        "    result_table[i, 3] = sum(rand_forest_test_acc) / NUM_TRIALS\n",
        "\n",
        "    # result_table1[i, 0] = best_train0\n",
        "    result_table1[i, 1] = best_train1\n",
        "    result_table1[i, 2] = best_train2\n",
        "    result_table1[i, 3] = best_train3\n",
        "\n",
        "    # result_table2[i, 0] = C0\n",
        "    result_table2[i, 1] = C1\n",
        "    result_table2[i, 2] = C2\n",
        "    result_table2[i, 3] = C3\n",
        "\n",
        "    # Average all test accuracies for all 3 trials\n",
        "    print(\"Test Accuracy Average for knn = \", sum(knn_test_acc) / NUM_TRIALS)\n",
        "    print(\"Test Accuracy Average for Random Forest = \", sum(rand_forest_test_acc) / NUM_TRIALS)\n",
        "    print(\"Test Accuracy Average for Decision Tree = \", sum(decision_tree_test_acc) / NUM_TRIALS)\n",
        "    print(\"Test Accuracy Average for SVM = \", sum(svm_test_acc)/NUM_TRIALS)\n",
        "\n",
        "    # y-axis: partition\n",
        "    # x-axis: classifier\n",
        "    print(result_table)\n",
        "    print(\"############################\")\n",
        "    print(result_table1)\n",
        "    print(\"############################\")\n",
        "    print(result_table2)\n",
        "\n",
        "# Visualize the results\n",
        "classifiers = ['KNN', 'Decision Tree', 'Random Forest']\n",
        "x = np.arange(len(classifiers))\n",
        "width = 0.2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "for i, partition in enumerate(partitionVal):\n",
        "    ax.bar(x + i * width, result_table[i, 1:4], width, label=f'Partition {partition}')\n",
        "\n",
        "ax.set_xticks(x + width / 2)\n",
        "ax.set_xticklabels(classifiers)\n",
        "ax.set_xlabel('Classifier')\n",
        "ax.set_ylabel('Average Test Accuracy')\n",
        "ax.set_title('Performance of Different Classifiers for Bankruptcy Prediction')\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsao6esgKdkd"
      },
      "source": [
        "## **Students Performance Dataset**\n",
        "\n",
        "This given dataset provides 8 various attributes that give us the necessary information to predict how a student might score on in math, reading, and writing. The reasoning behind using this dataset is for a basis for a real world scenario, where a schools want to see how students are doing based on their background.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Students Performance Dataset: Dealing with outliers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom MultiColumnLabelEncoder class\n",
        "class MultiColumnLabelEncoder:\n",
        "    def __init__(self, columns=None):\n",
        "        self.columns = columns  # array of column names to encode\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self  # not relevant here\n",
        "\n",
        "    def transform(self, X):\n",
        "        '''\n",
        "        Transforms columns of X specified in self.columns using\n",
        "        LabelEncoder(). If no columns specified, transforms all\n",
        "        columns in X.\n",
        "        '''\n",
        "        output = X.copy()\n",
        "        if self.columns is not None:\n",
        "            for col in self.columns:\n",
        "                output[col] = LabelEncoder().fit_transform(output[col])\n",
        "        else:\n",
        "            for colname, col in output.iteritems():\n",
        "                output[colname] = LabelEncoder().fit_transform(col)\n",
        "        return output\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        return self.fit(X, y).transform(X)\n",
        "\n",
        "# Load the dataset\n",
        "students_data_preserved = pd.read_csv('StudentsPerformance.csv')\n",
        "students_data = students_data_preserved.copy()  # Create a copy for manipulation\n",
        "\n",
        "# Basic cleanup\n",
        "students_data.dropna(inplace=True)\n",
        "print(students_data.shape)\n",
        "print(students_data.head())\n",
        "\n",
        "# Identify numerical and categorical features\n",
        "numerical_features = students_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = students_data.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Separate target variable from features\n",
        "target_column = 'math score'  # Replace with the desired target variable\n",
        "X = students_data.drop(target_column, axis=1)\n",
        "y = students_data[target_column]\n",
        "\n",
        "# Remove the target column from numerical features\n",
        "numerical_features = numerical_features.drop(target_column)\n",
        "\n",
        "# Detect and handle outliers\n",
        "def detect_outliers(data, features):\n",
        "    outlier_indices = []\n",
        "    for feature in features:\n",
        "        z_scores = stats.zscore(data[feature])\n",
        "        outlier_indices.extend(data.loc[np.abs(z_scores) > 3, feature].index)\n",
        "    return list(set(outlier_indices))\n",
        "\n",
        "outlier_indices = detect_outliers(X, numerical_features)\n",
        "X = X.drop(X.index[outlier_indices])\n",
        "y = y.drop(y.index[outlier_indices])\n",
        "\n",
        "# Encode categorical features using the custom MultiColumnLabelEncoder\n",
        "categorical_transformer = MultiColumnLabelEncoder(columns=categorical_features)\n",
        "X_categorical_encoded = categorical_transformer.fit_transform(X[categorical_features])\n",
        "\n",
        "# Encode numerical features using StandardScaler\n",
        "numerical_transformer = StandardScaler()\n",
        "X_numerical = X[numerical_features]\n",
        "X_numerical_scaled = numerical_transformer.fit_transform(X_numerical)\n",
        "\n",
        "# Combine encoded categorical and scaled numerical features\n",
        "X_encoded = np.concatenate([X_categorical_encoded, X_numerical_scaled], axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vLzzL1i1UaXt",
        "outputId": "f53ca155-5de4-4159-8b36-fc25bffbd7b1"
      },
      "outputs": [],
      "source": [
        "# SVM function\n",
        "def svm_func(X_train, X_test, y_train, y_test):\n",
        "    # SVM binary classification used linear instead of RBF (Faster runtime)\n",
        "    classifier = svm.SVC(kernel='linear')\n",
        "\n",
        "    # Different C values to try\n",
        "    C_list = [10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 1]\n",
        "    parameters = {'C': C_list}\n",
        "\n",
        "    # Perform a grid Search to identify the best C and to perform K 5 Folds\n",
        "    clf = GridSearchCV(classifier, parameters, return_train_score=True, cv=5)\n",
        "\n",
        "    # Have to fit the classifier with the training data\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Extract the training and validation accuracies and plot them as heat maps\n",
        "    # to visualize the best C parameter\n",
        "    train_acc = clf.cv_results_['mean_train_score']\n",
        "    val_acc = clf.cv_results_['mean_test_score']\n",
        "\n",
        "    # Find the optimal C parameter and use that to redefine the classifier\n",
        "    optimal_classifier = svm.SVC(kernel='linear', C=clf.best_params_['C'])\n",
        "\n",
        "    for i, j in enumerate(C_list):\n",
        "        if j == clf.best_params_['C']:\n",
        "            best_train_acc = train_acc[i]\n",
        "\n",
        "    # Find test accuracy\n",
        "    optimal_classifier.fit(X_train, y_train)\n",
        "    test_acc = optimal_classifier.score(X_test, y_test)\n",
        "    return test_acc, best_train_acc, clf.best_params_['C']\n",
        "\n",
        "# KNN function\n",
        "def knn_classifier(X_train, X_test, y_train, y_test):\n",
        "    k_list = np.array([1, 2, 3, 4, 5, 6])\n",
        "    parameters = {'n_neighbors': k_list}\n",
        "    classifier_grid = GridSearchCV(KNeighborsClassifier(), parameters, cv=5, return_train_score=True)\n",
        "    classifier_grid.fit(X_train, y_train)\n",
        "\n",
        "    # Plot heatmaps for the Training and Testing scores respectively\n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score'].reshape(6, 1)\n",
        "    val_acc = classifier_grid.cv_results_['mean_test_score'].reshape(6, 1)\n",
        "\n",
        "    # Train and Test with best parameters\n",
        "    k_star = classifier_grid.best_params_['n_neighbors']\n",
        "    classifier_test = KNeighborsClassifier(n_neighbors=k_star)\n",
        "    classifier_test.fit(X_train, y_train)\n",
        "    knn_acc = classifier_test.score(X_test, y_test)\n",
        "\n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score']\n",
        "    for i, j in enumerate(k_list):\n",
        "        if j == k_star:\n",
        "            best_train_acc = train_acc[i]\n",
        "\n",
        "    return knn_acc, best_train_acc, k_star\n",
        "\n",
        "# Decision Tree function\n",
        "def decision_Tree(X_train, X_test, y_train, y_test):\n",
        "    D_list = np.array([1, 2, 3, 4, 5])\n",
        "    parameters = {'max_depth': D_list}\n",
        "    classifier_grid = GridSearchCV(DecisionTreeClassifier(criterion=\"entropy\"), parameters, cv=5, return_train_score=True)\n",
        "    classifier_grid.fit(X_train, y_train)\n",
        "\n",
        "    # Show the heatmaps\n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score'].reshape(5, 1)\n",
        "    val_acc = classifier_grid.cv_results_['mean_test_score'].reshape(5, 1)\n",
        "\n",
        "    # Train and Test with best parameters\n",
        "    D_star = classifier_grid.best_params_['max_depth']\n",
        "    classifier_test = DecisionTreeClassifier(max_depth=D_star, criterion=\"entropy\")\n",
        "    classifier_test.fit(X_train, y_train)\n",
        "    decision_tree_acc = classifier_test.score(X_test, y_test)\n",
        "\n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score']\n",
        "    for i, j in enumerate(D_list):\n",
        "        if j == D_star:\n",
        "            best_train_acc = train_acc[i]\n",
        "\n",
        "    return decision_tree_acc, best_train_acc, D_star\n",
        "\n",
        "# Random Forest function\n",
        "def rand_Forest(X_train, X_test, y_train, y_test):\n",
        "    D_list = np.array([1, 2, 3, 4, 5])\n",
        "    parameters = {'max_depth': D_list}\n",
        "    classifier_grid = GridSearchCV(RandomForestClassifier(criterion=\"entropy\"), parameters, cv=5, return_train_score=True)\n",
        "    classifier_grid.fit(X_train, y_train)\n",
        "\n",
        "    # Show Heatmaps\n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score'].reshape(5, 1)\n",
        "    val_acc = classifier_grid.cv_results_['mean_test_score'].reshape(5, 1)\n",
        "\n",
        "    # Train and Test with best parameters\n",
        "    D_star = classifier_grid.best_params_['max_depth']\n",
        "    classifier_test = RandomForestClassifier(max_depth=D_star, criterion=\"entropy\")\n",
        "    classifier_test.fit(X_train, y_train)\n",
        "    random_forest_acc = classifier_test.score(X_test, y_test)\n",
        "\n",
        "    train_acc = classifier_grid.cv_results_['mean_train_score']\n",
        "    for i, j in enumerate(D_list):\n",
        "        if j == D_star:\n",
        "            best_train_acc = train_acc[i]\n",
        "\n",
        "    return random_forest_acc, best_train_acc, D_star\n",
        "\n",
        "partitionVal = [0.8, 0.5, 0.2]\n",
        "result_table = np.zeros((3, 7))\n",
        "result_table1 = np.zeros((3, 7))\n",
        "result_table2 = np.zeros((3, 7))\n",
        "\n",
        "for i, partition in enumerate(partitionVal):\n",
        "    print(\"Partition: \", partition)\n",
        "    knn_test_acc = []\n",
        "    rand_forest_test_acc = []\n",
        "    decision_tree_test_acc = []\n",
        "    svm_test_acc = []\n",
        "\n",
        "    NUM_TRIALS = 5\n",
        "    for trial in range(NUM_TRIALS):\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=1-partition, random_state=42)\n",
        "\n",
        "        test_acc, best_train0, C0 = svm_func(X_train, X_test, y_train, y_test)\n",
        "        svm_test_acc.append(test_acc)\n",
        "\n",
        "        test_acc, best_train1, C1 = knn_classifier(X_train, X_test, y_train, y_test)\n",
        "        knn_test_acc.append(test_acc)\n",
        "\n",
        "        test_acc, best_train2, C2 = decision_Tree(X_train, X_test, y_train, y_test)\n",
        "        decision_tree_test_acc.append(test_acc)\n",
        "\n",
        "        test_acc, best_train3, C3 = rand_Forest(X_train, X_test, y_train, y_test)\n",
        "        rand_forest_test_acc.append(test_acc)\n",
        "\n",
        "    result_table[i, 0] = sum(svm_test_acc) / NUM_TRIALS\n",
        "    result_table[i, 1] = sum(knn_test_acc) / NUM_TRIALS\n",
        "    result_table[i, 2] = sum(decision_tree_test_acc) / NUM_TRIALS\n",
        "    result_table[i, 3] = sum(rand_forest_test_acc) / NUM_TRIALS\n",
        "\n",
        "    result_table1[i, 0] = best_train0\n",
        "    result_table1[i, 1] = best_train1\n",
        "    result_table1[i, 2] = best_train2\n",
        "    result_table1[i, 3] = best_train3\n",
        "\n",
        "    result_table2[i, 0] = C0\n",
        "    result_table2[i, 1] = C1\n",
        "    result_table2[i, 2] = C2\n",
        "    result_table2[i, 3] = C3\n",
        "\n",
        "    # Print average test accuracy for each model\n",
        "    print(\"Test Accuracy Average for SVM = \", sum(svm_test_acc) / NUM_TRIALS)\n",
        "    print(\"Test Accuracy Average for knn = \", sum(knn_test_acc) / NUM_TRIALS)\n",
        "    print(\"Test Accuracy Average for Random Forest = \", sum(rand_forest_test_acc) / NUM_TRIALS)\n",
        "    print(\"Test Accuracy Average for Decision Tree = \", sum(decision_tree_test_acc) / NUM_TRIALS)\n",
        "\n",
        "    # y-axis: partition\n",
        "    # x-axis: classifier\n",
        "    print(result_table)\n",
        "    print(\"############################\")\n",
        "    print(result_table1)\n",
        "    print(\"############################\")\n",
        "    print(result_table2)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "COGS118A_FINAL.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
